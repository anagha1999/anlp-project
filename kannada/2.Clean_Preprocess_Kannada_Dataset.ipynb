{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anagha1999/anlp-project/blob/main/kannada/2.Clean_Preprocess_Kannada_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anagha1999/anlp-project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ikjyd24qz9v",
        "outputId": "7b6dc18c-5e54-4f15-90e5-7e610127e00f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anlp-project'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 170 (delta 56), reused 101 (delta 27), pack-reused 23 (from 1)\u001b[K\n",
            "Receiving objects: 100% (170/170), 39.37 MiB | 19.24 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55835c53",
        "outputId": "49944b6a-efad-4969-f2eb-6c24cb850fe7"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/crvineeth97/kannada-stop-words/refs/heads/master/stop-words.txt\"\n",
        "]\n",
        "\n",
        "for url in urls:\n",
        "    filename = url.split('/')[-1]\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded: {filename}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: stop-words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7027f095"
      },
      "source": [
        "# Task\n",
        "Use `pypdf` to extract text from specific pages of the following PDF files: pages 17-236 for \"1-janapada-kathegalu.pdf\" and pages 10-225 for \"Satrii_Niiti_Kathegal'u_text.pdf\". Then, load the stop words from \"stop-words.txt\", remove these words from the extracted texts, and save the cleaned content into a new directory named `kannada-pre-processed-texts` with the filenames `1-janapada-kathegalu.txt` and `Satrii_Niiti_Kathegal'u_text.txt`. Finally, summarize the extraction and cleaning process and confirm the paths of the saved files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67c94eee"
      },
      "source": [
        "## Extract Text from PDFs\n",
        "\n",
        "### Subtask:\n",
        "Install `pypdf` and extract text from the specified page ranges of the two PDF files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f22373c3"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `pypdf` library to enable PDF text extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55110b35",
        "outputId": "46c1011d-85a6-489b-ae8d-3fb9a530e9c5"
      },
      "source": [
        "!pip install pypdf"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c4105d5"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract text from the specified pages of the PDF files using `pypdf` and store the results in a dictionary, then print the first 500 characters of each extracted text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566cdbd6",
        "outputId": "7f7a51bc-75a7-43d9-bb4e-b320529cee26"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Define filenames and page ranges\n",
        "pdf_configs = [\n",
        "    {'filename': '1-janapada-kathegalu.pdf', 'start': 17, 'end': 236},\n",
        "    {'filename': \"2-Satrii_Niiti_Kathegal'u_text.pdf\", 'start': 10, 'end': 225}\n",
        "]\n",
        "\n",
        "# Directory where the repo was cloned\n",
        "repo_dir = 'anlp-project/kannada/kannada-dataset'\n",
        "\n",
        "extracted_texts = {}\n",
        "\n",
        "for config in pdf_configs:\n",
        "    filename = config['filename']\n",
        "    start_page = config['start']\n",
        "    end_page = config['end']\n",
        "\n",
        "    # 1. Try to find the file in the repo directory\n",
        "    file_path = os.path.join(repo_dir, filename)\n",
        "\n",
        "    # 2. If not found, check current directory\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = filename\n",
        "\n",
        "    # 3. If still invalid or corrupt, download it\n",
        "    if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n",
        "        print(f\"File {filename} missing or corrupt. Downloading...\")\n",
        "        url = f\"https://github.com/anagha1999/anlp-project/raw/main/kannada-dataset/{filename}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            file_path = filename\n",
        "        else:\n",
        "             print(f\"Failed to download {filename}\")\n",
        "\n",
        "    print(f\"Processing file: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text_content = []\n",
        "        # Convert 1-based page numbers to 0-based indices\n",
        "        # range(start, end) in python excludes end, so we need end_page (since we want up to end_page)\n",
        "        # Instructions say: 1-janapada-kathegalu.pdf: pages 17 to 236.\n",
        "        # Python index for page 17 is 16.\n",
        "        # We want to include page 236. So range should go up to 236 (index 235).\n",
        "        # range(16, 236) covers indices 16 to 235.\n",
        "        for i in range(start_page - 1, end_page):\n",
        "            if i < len(reader.pages):\n",
        "                page = reader.pages[i]\n",
        "                text_content.append(page.extract_text())\n",
        "\n",
        "        extracted_texts[filename] = \"\\n\".join(text_content)\n",
        "        print(f\"Successfully extracted text from {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {file_path}: {e}\")\n",
        "\n",
        "# Print the first 500 characters of each extracted text\n",
        "for filename, text in extracted_texts.items():\n",
        "    print(f\"\\n--- Start of {filename} ---\")\n",
        "    print(text[:500])\n",
        "    print(f\"--- End of Preview ---\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: anlp-project/kannada/kannada-dataset/1-janapada-kathegalu.pdf\n",
            "Successfully extracted text from 1-janapada-kathegalu.pdf\n",
            "Processing file: anlp-project/kannada/kannada-dataset/2-Satrii_Niiti_Kathegal'u_text.pdf\n",
            "Successfully extracted text from 2-Satrii_Niiti_Kathegal'u_text.pdf\n",
            "\n",
            "--- Start of 1-janapada-kathegalu.pdf ---\n",
            "೧. ಮುತುಗದ ಎಲೆ ಗೋತಾಯಿ \n",
            "ಒಂದ್‌ ಪಟ್ಟಣ. ಒಬ್ಬ ದೊಡ್ಡ ದೊರೆ. ಆ ದೊಡ್ಡ ದೊರೆಗೆ ಏಳ್‌ ಜನ ಮಕ್ಕಳು. ಆ \n",
            "ಏಳ್‌ ಜನ ಮಕ್ಕಳಿಗೂ ಚೆನ್ನಾಗಿ ಓದ್ದಿ, ಓದ್‌ ಬಂದೋರ್‌ಗೆ ಹಾಲು ತುಪ್ಪ ಅನ್ನ ಹಾಕಿ \n",
            "ಮಂಚದ ಮೇಲೆ ಕೂಡ್ರಿಸಿ ಚೆನ್ನಾಗಿ ಸಾಕೋರು. ಏಳು ಜನ ಮಕ್ಕಳೂವೆ ಹೆಣ್‌ ಮಕ್ಕಳೇ \n",
            "ಹುಟ್ಟಿದ್ದಾರಲ್ಲ ಅಂತ, ಆ ದೊರೆ ಒಂದಿನ ಹೆಣ್‌ಮಕ್ಕಳನ್ನೆಲ್ಲ ಕೇಳಿದನು, ನನ್ನಾರವ್ವ \n",
            "ಸಾಕೋರು ಅಂತ. ಆರು ಜನ ಮಕ್ಕಳೂವೆ ನಾನ್‌ ಸಾಕ್ತೀನಿ, ನಾನ್‌ ಸಾಕ್ತೀನಿಅಂತ ಅಂದ್ರು. \n",
            "ಆದರೆ ಕೊನೆ ಕಿರಿಮಗಳು ಮಾತ್ರ ನಾನ್‌ ಸಾಕಾಕಿಲ್ಲ ಅಂದ್ಲು. ಅಪ್ಪ ನಾವ್‌ ಸಾಕಾಕೆ ಆಗ್ಕದ? \n",
            "ಭಗವಂತ ನಮ್ಮನ್ನೆಲ್ಲ ಸಾಕ್‌ಬೇಕು. ಈ ಮಾತನ್ನೆ ಹೇಳ್ಕೊಂಡು ಬಂದ್ಲು. ಆಗ ಆ\n",
            "--- End of Preview ---\n",
            "\n",
            "--- Start of 2-Satrii_Niiti_Kathegal'u_text.pdf ---\n",
            "ಸ್ತ್ರೀ ನೀತಿ ಕಥೆಗಳು \n",
            "ಪುರಾತನ ಕಾಲದಲ್ಲಿ ಗೋಪಾಲಸೆ ಟ್ಟ ಎಂಬೊಬ್ಬ ವ್ಯಾಪಾರಿಯು \n",
            "ಕಾಶಿ ರ ನಗರದಲ್ಲಿ ವಾಸಿಸುತ್ತಿದ್ದರು. ಶೇಮಂತನಾದ ಈ ವರ್ತಕನಿಗೆ \n",
            "ಮೋಹಿನಿಯೊಬ್ಬ ಳೇ ಮಗಳು. ರೊಪ ಸುಂದರಿಯಾದ ಈ ಮೋಹಿನಿಯು \n",
            "ಓದುಬರಹದಲ್ಲೂ ಅಕಿ ನಿಪುಣಳು. ಅವಳು ವಯಸ್ಸಿಗೆ ಬಂದ ಕೂಡಲೇ \n",
            "ತಂದೆಯು ಮದುವೆ ಮಾಡಿಕೊಟ್ಟ ನು. ಮೋಹಿನಿಯ ಪತಿಯು ಅನುರೂಪ \n",
            "ನೇನೋ ನಿಜ, ಆದರೆ ವಿದ್ಯಾ ಬುದ್ಧಿ ಯುತ. ಸ್ವಲ್ಪ ಲ, ಮೂಢನೆಂದರೂ \n",
            "ಸರಿಯೇ. ಇದರಿಂದ ಅವಳಿಗೆ ಸುಖ ದೊರೆಯಲಿಲ್ಲ. ಅವಳು ಚಿಂತೆಯಿಂದ \n",
            "ಕೊರಗಲಾರಂಭಿಸಿದಳು. ದಿನದಿಂದ ದಿನಕ್ಕೆ ಕೃಷವಾಗುತ್ತ ಬಂದಳು. \n",
            "ಕೊನೆಗೆ ಅದೊಂದು ರೋಗವಾಗಿಯೇ ಪರಿಣಮಿಸಿತು, ಇದನ್ನು ಕಂಡ \n",
            "ಮೋಹಿನಿಯ ಗಂಡನು ಅನೇಕ ವೈದ್\n",
            "--- End of Preview ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8269ac44"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the stop words from the text file, create the output directory, clean the extracted texts by removing stop words, and save the cleaned texts to the specified files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9ee8731",
        "outputId": "1decfe8d-1197-4118-a266-d1ea7fe62ff6"
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Load stop words\n",
        "with open('stop-words.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = set(f.read().split())\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'kannada-pre-processed'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "cleaned_files = []\n",
        "\n",
        "# Process each extracted text\n",
        "for filename, text in extracted_texts.items():\n",
        "    # First, replace multiple consecutive newlines (paragraph breaks) with '---\\n'\n",
        "    # This ensures paragraph separation happens before stop word removal.\n",
        "    text_with_paragraphs = re.sub(r'\\n\\s*\\n', '\\n---\\n', text)\n",
        "\n",
        "    # Now, process line by line to remove stop words\n",
        "    cleaned_lines = []\n",
        "    for line in text_with_paragraphs.splitlines():\n",
        "        words = line.split()\n",
        "        cleaned_words = [w for w in words if w not in stop_words]\n",
        "        cleaned_lines.append(\" \".join(cleaned_words))\n",
        "\n",
        "    # The final cleaned text will have stop words removed and paragraphs separated\n",
        "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
        "\n",
        "    # Determine output filename (change extension to .txt)\n",
        "    base_name = os.path.basename(filename)\n",
        "    txt_filename = os.path.splitext(base_name)[0] + \".txt\"\n",
        "    output_path = os.path.join(output_dir, txt_filename)\n",
        "\n",
        "    # Save the cleaned text\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(cleaned_text)\n",
        "\n",
        "    cleaned_files.append(output_path)\n",
        "\n",
        "print(\"Extraction and cleaning process completed (structure preserved).\")\n",
        "print(\"Saved files at:\")\n",
        "for path in cleaned_files:\n",
        "    print(path)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction and cleaning process completed (structure preserved).\n",
            "Saved files at:\n",
            "kannada-pre-processed/1-janapada-kathegalu.txt\n",
            "kannada-pre-processed/2-Satrii_Niiti_Kathegal'u_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Paragraph Embeddings for each Text [Kannada]\n",
        "\n",
        "### Subtask:\n",
        "Read text files, split them into paragraphs, generate embeddings using the loaded model, and store them in a dictionary.\n"
      ],
      "metadata": {
        "id": "FA3539RsJn3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Define the file path\n",
        "file_path_to_check = os.path.join('anlp-project', 'kannada', 'kannada-pre-processed', '1-janapada-kathegalu.txt')\n",
        "\n",
        "# Ensure the file exists before proceeding\n",
        "if os.path.exists(file_path_to_check):\n",
        "    # 2. Open the file in read mode ('r') with UTF-8 encoding\n",
        "    with open(file_path_to_check, 'r', encoding='utf-8') as f:\n",
        "        # 3. Read the entire content of the file into a string variable\n",
        "        content = f.read()\n",
        "\n",
        "    # 4. Count paragraphs based on \"---\" separator\n",
        "    paragraphs = [p.strip() for p in content.split('---') if p.strip()]\n",
        "\n",
        "    # 5. Print the count\n",
        "    print(f\"File: {file_path_to_check}\")\n",
        "    print(f\"Total Paragraphs (separated by '---'): {len(paragraphs)}\")\n",
        "\n",
        "    if paragraphs:\n",
        "        print(f\"First paragraph start: {paragraphs[0][:50]}...\")\n",
        "else:\n",
        "    print(f\"Error: File not found at {file_path_to_check}\")"
      ],
      "metadata": {
        "id": "WFaS-AqpypLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31768425-4f35-4f09-ae22-29a0ca3250cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: anlp-project/kannada/kannada-pre-processed/1-janapada-kathegalu.txt\n",
            "Total Paragraphs (separated by '---'): 220\n",
            "First paragraph start: ೧. ಮುತುಗದ ಎಲೆ ಗೋತಾಯಿ\n",
            "ಒಂದ್‌ ಪಟ್ಟಣ. ಒಬ್ಬ ದೊಡ್ಡ ದೊರೆ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "EMBEDDINGS_MODEL_NAME='l3cube-pune/indic-sentence-similarity-sbert'\n",
        "model = SentenceTransformer(EMBEDDINGS_MODEL_NAME)"
      ],
      "metadata": {
        "id": "uNx_fzOaKDsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "target_dir = 'anlp-project/kannada/kannada-pre-processed'\n",
        "file_embeddings = {}\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    for filename in os.listdir(target_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            filepath = os.path.join(target_dir, filename)\n",
        "\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Split content into paragraphs using '---' as separator and filter empty strings\n",
        "            paragraphs = [p.strip() for p in content.split('---') if p.strip()]\n",
        "\n",
        "            if paragraphs:\n",
        "                # Generate embeddings\n",
        "                embeddings = model.encode(paragraphs)\n",
        "                file_embeddings[filename] = embeddings\n",
        "\n",
        "                print(f\"File: {filename}\")\n",
        "                print(f\"  Paragraphs processed: {len(paragraphs)}\")\n",
        "                print(f\"  Embeddings shape: {embeddings.shape}\")\n",
        "            else:\n",
        "                print(f\"File: {filename} - No paragraphs found.\")\n",
        "else:\n",
        "    print(f\"Target directory not found: {target_dir}\")"
      ],
      "metadata": {
        "id": "MFf6GdTUJ4rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define the file path for saving the embeddings\n",
        "embeddings_file_path = 'kannada_texts_embeddings.pkl'\n",
        "\n",
        "# Save the file_embeddings dictionary to a pickle file\n",
        "with open(embeddings_file_path, 'wb') as f:\n",
        "    pickle.dump(file_embeddings, f)\n",
        "\n",
        "print(f\"Successfully saved paragraph embeddings to {embeddings_file_path}\")"
      ],
      "metadata": {
        "id": "Dm5c5UtrKJqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}