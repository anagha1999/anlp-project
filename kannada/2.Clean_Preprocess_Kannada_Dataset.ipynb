{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anagha1999/anlp-project/blob/main/kannada/2.Clean_Preprocess_Kannada_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ikjyd24qz9v",
        "outputId": "7b6dc18c-5e54-4f15-90e5-7e610127e00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'anlp-project'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 170 (delta 56), reused 101 (delta 27), pack-reused 23 (from 1)\u001b[K\n",
            "Receiving objects: 100% (170/170), 39.37 MiB | 19.24 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/anagha1999/anlp-project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55835c53",
        "outputId": "49944b6a-efad-4969-f2eb-6c24cb850fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded: stop-words.txt\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/crvineeth97/kannada-stop-words/refs/heads/master/stop-words.txt\"\n",
        "]\n",
        "\n",
        "for url in urls:\n",
        "    filename = url.split('/')[-1]\n",
        "    response = requests.get(url)\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7027f095"
      },
      "source": [
        "## Extract Text from PDFs\n",
        "Use `pypdf` to extract text from specific pages of the following PDF files: pages 17-236 for \"1-janapada-kathegalu.pdf\" and pages 10-225 for \"Satrii_Niiti_Kathegal'u_text.pdf\". Then, load the stop words from \"stop-words.txt\", remove these words from the extracted texts, and save the cleaned content into a new directory named `kannada-pre-processed-texts` with the filenames `1-janapada-kathegalu.txt` and `Satrii_Niiti_Kathegal'u_text.txt`. Finally, summarize the extraction and cleaning process and confirm the paths of the saved files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55110b35",
        "outputId": "46c1011d-85a6-489b-ae8d-3fb9a530e9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566cdbd6",
        "outputId": "7f7a51bc-75a7-43d9-bb4e-b320529cee26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: anlp-project/kannada/kannada-dataset/1-janapada-kathegalu.pdf\n",
            "Successfully extracted text from 1-janapada-kathegalu.pdf\n",
            "Processing file: anlp-project/kannada/kannada-dataset/2-Satrii_Niiti_Kathegal'u_text.pdf\n",
            "Successfully extracted text from 2-Satrii_Niiti_Kathegal'u_text.pdf\n",
            "\n",
            "--- Start of 1-janapada-kathegalu.pdf ---\n",
            "೧. ಮುತುಗದ ಎಲೆ ಗೋತಾಯಿ \n",
            "ಒಂದ್‌ ಪಟ್ಟಣ. ಒಬ್ಬ ದೊಡ್ಡ ದೊರೆ. ಆ ದೊಡ್ಡ ದೊರೆಗೆ ಏಳ್‌ ಜನ ಮಕ್ಕಳು. ಆ \n",
            "ಏಳ್‌ ಜನ ಮಕ್ಕಳಿಗೂ ಚೆನ್ನಾಗಿ ಓದ್ದಿ, ಓದ್‌ ಬಂದೋರ್‌ಗೆ ಹಾಲು ತುಪ್ಪ ಅನ್ನ ಹಾಕಿ \n",
            "ಮಂಚದ ಮೇಲೆ ಕೂಡ್ರಿಸಿ ಚೆನ್ನಾಗಿ ಸಾಕೋರು. ಏಳು ಜನ ಮಕ್ಕಳೂವೆ ಹೆಣ್‌ ಮಕ್ಕಳೇ \n",
            "ಹುಟ್ಟಿದ್ದಾರಲ್ಲ ಅಂತ, ಆ ದೊರೆ ಒಂದಿನ ಹೆಣ್‌ಮಕ್ಕಳನ್ನೆಲ್ಲ ಕೇಳಿದನು, ನನ್ನಾರವ್ವ \n",
            "ಸಾಕೋರು ಅಂತ. ಆರು ಜನ ಮಕ್ಕಳೂವೆ ನಾನ್‌ ಸಾಕ್ತೀನಿ, ನಾನ್‌ ಸಾಕ್ತೀನಿಅಂತ ಅಂದ್ರು. \n",
            "ಆದರೆ ಕೊನೆ ಕಿರಿಮಗಳು ಮಾತ್ರ ನಾನ್‌ ಸಾಕಾಕಿಲ್ಲ ಅಂದ್ಲು. ಅಪ್ಪ ನಾವ್‌ ಸಾಕಾಕೆ ಆಗ್ಕದ? \n",
            "ಭಗವಂತ ನಮ್ಮನ್ನೆಲ್ಲ ಸಾಕ್‌ಬೇಕು. ಈ ಮಾತನ್ನೆ ಹೇಳ್ಕೊಂಡು ಬಂದ್ಲು. ಆಗ ಆ\n",
            "--- End of Preview ---\n",
            "\n",
            "--- Start of 2-Satrii_Niiti_Kathegal'u_text.pdf ---\n",
            "ಸ್ತ್ರೀ ನೀತಿ ಕಥೆಗಳು \n",
            "ಪುರಾತನ ಕಾಲದಲ್ಲಿ ಗೋಪಾಲಸೆ ಟ್ಟ ಎಂಬೊಬ್ಬ ವ್ಯಾಪಾರಿಯು \n",
            "ಕಾಶಿ ರ ನಗರದಲ್ಲಿ ವಾಸಿಸುತ್ತಿದ್ದರು. ಶೇಮಂತನಾದ ಈ ವರ್ತಕನಿಗೆ \n",
            "ಮೋಹಿನಿಯೊಬ್ಬ ಳೇ ಮಗಳು. ರೊಪ ಸುಂದರಿಯಾದ ಈ ಮೋಹಿನಿಯು \n",
            "ಓದುಬರಹದಲ್ಲೂ ಅಕಿ ನಿಪುಣಳು. ಅವಳು ವಯಸ್ಸಿಗೆ ಬಂದ ಕೂಡಲೇ \n",
            "ತಂದೆಯು ಮದುವೆ ಮಾಡಿಕೊಟ್ಟ ನು. ಮೋಹಿನಿಯ ಪತಿಯು ಅನುರೂಪ \n",
            "ನೇನೋ ನಿಜ, ಆದರೆ ವಿದ್ಯಾ ಬುದ್ಧಿ ಯುತ. ಸ್ವಲ್ಪ ಲ, ಮೂಢನೆಂದರೂ \n",
            "ಸರಿಯೇ. ಇದರಿಂದ ಅವಳಿಗೆ ಸುಖ ದೊರೆಯಲಿಲ್ಲ. ಅವಳು ಚಿಂತೆಯಿಂದ \n",
            "ಕೊರಗಲಾರಂಭಿಸಿದಳು. ದಿನದಿಂದ ದಿನಕ್ಕೆ ಕೃಷವಾಗುತ್ತ ಬಂದಳು. \n",
            "ಕೊನೆಗೆ ಅದೊಂದು ರೋಗವಾಗಿಯೇ ಪರಿಣಮಿಸಿತು, ಇದನ್ನು ಕಂಡ \n",
            "ಮೋಹಿನಿಯ ಗಂಡನು ಅನೇಕ ವೈದ್\n",
            "--- End of Preview ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Define filenames and page ranges\n",
        "pdf_configs = [\n",
        "    {'filename': '1-janapada-kathegalu.pdf', 'start': 17, 'end': 236},\n",
        "    {'filename': \"2-Satrii_Niiti_Kathegal'u_text.pdf\", 'start': 10, 'end': 225}\n",
        "]\n",
        "\n",
        "# Directory where the repo was cloned\n",
        "repo_dir = 'anlp-project/kannada/kannada-dataset'\n",
        "\n",
        "extracted_texts = {}\n",
        "\n",
        "for config in pdf_configs:\n",
        "    filename = config['filename']\n",
        "    start_page = config['start']\n",
        "    end_page = config['end']\n",
        "\n",
        "    # 1. Try to find the file in the repo directory\n",
        "    file_path = os.path.join(repo_dir, filename)\n",
        "\n",
        "    # 2. If not found, check current directory\n",
        "    if not os.path.exists(file_path):\n",
        "        file_path = filename\n",
        "\n",
        "    # 3. If still invalid or corrupt, download it\n",
        "    if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n",
        "        print(f\"File {filename} missing or corrupt. Downloading...\")\n",
        "        url = f\"https://github.com/anagha1999/anlp-project/raw/main/kannada-dataset/{filename}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            file_path = filename\n",
        "        else:\n",
        "             print(f\"Failed to download {filename}\")\n",
        "\n",
        "    print(f\"Processing file: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text_content = []\n",
        "        # Convert 1-based page numbers to 0-based indices\n",
        "        # range(start, end) in python excludes end, so we need end_page (since we want up to end_page)\n",
        "        # Instructions say: 1-janapada-kathegalu.pdf: pages 17 to 236.\n",
        "        # Python index for page 17 is 16.\n",
        "        # We want to include page 236. So range should go up to 236 (index 235).\n",
        "        # range(16, 236) covers indices 16 to 235.\n",
        "        for i in range(start_page - 1, end_page):\n",
        "            if i < len(reader.pages):\n",
        "                page = reader.pages[i]\n",
        "                text_content.append(page.extract_text())\n",
        "\n",
        "        extracted_texts[filename] = \"\\n\".join(text_content)\n",
        "        print(f\"Successfully extracted text from {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {file_path}: {e}\")\n",
        "\n",
        "# Print the first 500 characters of each extracted text\n",
        "for filename, text in extracted_texts.items():\n",
        "    print(f\"\\n--- Start of {filename} ---\")\n",
        "    print(text[:500])\n",
        "    print(f\"--- End of Preview ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9ee8731",
        "outputId": "1decfe8d-1197-4118-a266-d1ea7fe62ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction and cleaning process completed (structure preserved).\n",
            "Saved files at:\n",
            "kannada-pre-processed/1-janapada-kathegalu.txt\n",
            "kannada-pre-processed/2-Satrii_Niiti_Kathegal'u_text.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# Load stop words\n",
        "with open('stop-words.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = set(f.read().split())\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'kannada-pre-processed'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "cleaned_files = []\n",
        "\n",
        "# Process each extracted text\n",
        "for filename, text in extracted_texts.items():\n",
        "    # First, replace multiple consecutive newlines (paragraph breaks) with '---\\n'\n",
        "    text_with_paragraphs = re.sub(r'\\n\\s*\\n', '\\n---\\n', text)\n",
        "    \n",
        "    # Split into paragraphs\n",
        "    raw_paragraphs = text_with_paragraphs.split('---')\n",
        "    \n",
        "    # Process each paragraph to remove stop words\n",
        "    cleaned_paragraphs = []\n",
        "    for para in raw_paragraphs:\n",
        "        # Process line by line within each paragraph\n",
        "        cleaned_lines = []\n",
        "        for line in para.splitlines():\n",
        "            words = line.split()\n",
        "            cleaned_words = [w for w in words if w not in stop_words]\n",
        "            cleaned_lines.append(\" \".join(cleaned_words))\n",
        "        \n",
        "        # Join lines back and strip whitespace\n",
        "        cleaned_para = \"\\n\".join(cleaned_lines).strip()\n",
        "        if cleaned_para:  # Only add non-empty paragraphs\n",
        "            cleaned_paragraphs.append(cleaned_para)\n",
        "\n",
        "    # Determine output filename (change extension to .csv)\n",
        "    base_name = os.path.basename(filename)\n",
        "    csv_filename = os.path.splitext(base_name)[0] + \".csv\"\n",
        "    output_path = os.path.join(output_dir, csv_filename)\n",
        "\n",
        "    # Save as CSV with paragraph_num and cleaned_text columns\n",
        "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['paragraph_num', 'cleaned_text'])\n",
        "        for idx, para in enumerate(cleaned_paragraphs, start=1):\n",
        "            writer.writerow([idx, para])\n",
        "\n",
        "    cleaned_files.append(output_path)\n",
        "\n",
        "print(\"Extraction and cleaning process completed.\")\n",
        "print(f\"Total files processed: {len(cleaned_files)}\")\n",
        "print(\"Saved files at:\")\n",
        "for path in cleaned_files:\n",
        "    print(f\"  {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA3539RsJn3i"
      },
      "source": [
        "## Generate Paragraph Embeddings for each Text [Kannada]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFaS-AqpypLI",
        "outputId": "31768425-4f35-4f09-ae22-29a0ca3250cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: anlp-project/kannada/kannada-pre-processed/1-janapada-kathegalu.txt\n",
            "Total Paragraphs (separated by '---'): 220\n",
            "First paragraph start: ೧. ಮುತುಗದ ಎಲೆ ಗೋತಾಯಿ\n",
            "ಒಂದ್‌ ಪಟ್ಟಣ. ಒಬ್ಬ ದೊಡ್ಡ ದೊರೆ....\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Define the file path\n",
        "file_path_to_check = os.path.join('anlp-project', 'kannada', 'kannada-pre-processed', '1-janapada-kathegalu.txt')\n",
        "\n",
        "# Ensure the file exists before proceeding\n",
        "if os.path.exists(file_path_to_check):\n",
        "    # 2. Open the file in read mode ('r') with UTF-8 encoding\n",
        "    with open(file_path_to_check, 'r', encoding='utf-8') as f:\n",
        "        # 3. Read the entire content of the file into a string variable\n",
        "        content = f.read()\n",
        "\n",
        "    # 4. Count paragraphs based on \"---\" separator\n",
        "    paragraphs = [p.strip() for p in content.split('---') if p.strip()]\n",
        "\n",
        "    # 5. Print the count\n",
        "    print(f\"File: {file_path_to_check}\")\n",
        "    print(f\"Total Paragraphs (separated by '---'): {len(paragraphs)}\")\n",
        "\n",
        "    if paragraphs:\n",
        "        print(f\"First paragraph start: {paragraphs[0][:50]}...\")\n",
        "else:\n",
        "    print(f\"Error: File not found at {file_path_to_check}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx_fzOaKDsY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "EMBEDDINGS_MODEL_NAME='l3cube-pune/indic-sentence-similarity-sbert'\n",
        "model = SentenceTransformer(EMBEDDINGS_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFf6GdTUJ4rG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "target_dir = 'anlp-project/kannada/kannada-pre-processed'\n",
        "file_embeddings = {}\n",
        "\n",
        "if os.path.exists(target_dir):\n",
        "    for filename in os.listdir(target_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            filepath = os.path.join(target_dir, filename)\n",
        "\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Split content into paragraphs using '---' as separator and filter empty strings\n",
        "            paragraphs = [p.strip() for p in content.split('---') if p.strip()]\n",
        "\n",
        "            if paragraphs:\n",
        "                # Generate embeddings\n",
        "                embeddings = model.encode(paragraphs)\n",
        "                file_embeddings[filename] = embeddings\n",
        "\n",
        "                print(f\"File: {filename}\")\n",
        "                print(f\"  Paragraphs processed: {len(paragraphs)}\")\n",
        "                print(f\"  Embeddings shape: {embeddings.shape}\")\n",
        "            else:\n",
        "                print(f\"File: {filename} - No paragraphs found.\")\n",
        "else:\n",
        "    print(f\"Target directory not found: {target_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm5c5UtrKJqd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Define the file path for saving the embeddings\n",
        "embeddings_file_path = 'kannada_texts_embeddings.pkl'\n",
        "\n",
        "# Save the file_embeddings dictionary to a pickle file\n",
        "with open(embeddings_file_path, 'wb') as f:\n",
        "    pickle.dump(file_embeddings, f)\n",
        "\n",
        "print(f\"Successfully saved paragraph embeddings to {embeddings_file_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
