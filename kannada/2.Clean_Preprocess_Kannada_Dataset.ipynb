{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anagha1999/anlp-project/blob/main/kannada/2.Clean_Preprocess_Kannada_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ikjyd24qz9v",
    "outputId": "7b6dc18c-5e54-4f15-90e5-7e610127e00f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'anlp-project'...\n",
      "remote: Enumerating objects: 170, done.\u001b[K\n",
      "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
      "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
      "remote: Total 170 (delta 56), reused 101 (delta 27), pack-reused 23 (from 1)\u001b[K\n",
      "Receiving objects: 100% (170/170), 39.37 MiB | 19.24 MiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/anagha1999/anlp-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.4-cp314-cp314-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp314-cp314-macosx_10_13_universal2.whl (207 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [requests]2/5\u001b[0m [charset_normalizer]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.11.12 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55835c53",
    "outputId": "49944b6a-efad-4969-f2eb-6c24cb850fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: stop-words.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/crvineeth97/kannada-stop-words/refs/heads/master/stop-words.txt\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    filename = url.split('/')[-1]\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7027f095"
   },
   "source": [
    "## Extract Text from PDFs\n",
    "Use `pypdf` to extract text from specific pages of the following PDF files: pages 17-236 for \"1-janapada-kathegalu.pdf\" and pages 10-225 for \"Satrii_Niiti_Kathegal'u_text.pdf\". Then, load the stop words from \"stop-words.txt\", remove these words from the extracted texts, and save the cleaned content into a new directory named `kannada-pre-processed-texts` with the filenames `1-janapada-kathegalu.txt` and `Satrii_Niiti_Kathegal'u_text.txt`. Finally, summarize the extraction and cleaning process and confirm the paths of the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55110b35",
    "outputId": "46c1011d-85a6-489b-ae8d-3fb9a530e9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /Users/anagha/Documents/fall2025/anlp-project/.venv/lib/python3.14/site-packages (6.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "566cdbd6",
    "outputId": "7f7a51bc-75a7-43d9-bb4e-b320529cee26"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Define filenames and page ranges\n",
    "pdf_configs = [\n",
    "    {'filename': '1-janapada-kathegalu.pdf', 'start': 17, 'end': 236},\n",
    "    {'filename': \"2-neeti-kathegalu.pdf\", 'start': 10, 'end': 225}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../kannada/kannada-dataset/1-janapada-kathegalu.pdf\n",
      "Successfully extracted text from 1-janapada-kathegalu.pdf\n",
      "Processing file: ../kannada/kannada-dataset/2-neeti-kathegalu.pdf\n",
      "Successfully extracted text from 2-neeti-kathegalu.pdf\n",
      "\n",
      "--- Start of 1-janapada-kathegalu.pdf ---\n",
      "೧. ಮುತುಗದ ಎಲೆ ಗೋತಾಯಿ \n",
      "ಒಂದ್‌ ಪಟ್ಟಣ. ಒಬ್ಬ ದೊಡ್ಡ ದೊರೆ. ಆ ದೊಡ್ಡ ದೊರೆಗೆ ಏಳ್‌ ಜನ ಮಕ್ಕಳು. ಆ \n",
      "ಏಳ್‌ ಜನ ಮಕ್ಕಳಿಗೂ ಚೆನ್ನಾಗಿ ಓದ್ದಿ, ಓದ್‌ ಬಂದೋರ್‌ಗೆ ಹಾಲು ತುಪ್ಪ ಅನ್ನ ಹಾಕಿ \n",
      "ಮಂಚದ ಮೇಲೆ ಕೂಡ್ರಿಸಿ ಚೆನ್ನಾಗಿ ಸಾಕೋರು. ಏಳು ಜನ ಮಕ್ಕಳೂವೆ ಹೆಣ್‌ ಮಕ್ಕಳೇ \n",
      "ಹುಟ್ಟಿದ್ದಾರಲ್ಲ ಅಂತ, ಆ ದೊರೆ ಒಂದಿನ ಹೆಣ್‌ಮಕ್ಕಳನ್ನೆಲ್ಲ ಕೇಳಿದನು, ನನ್ನಾರವ್ವ \n",
      "ಸಾಕೋರು ಅಂತ. ಆರು ಜನ ಮಕ್ಕಳೂವೆ ನಾನ್‌ ಸಾಕ್ತೀನಿ, ನಾನ್‌ ಸಾಕ್ತೀನಿಅಂತ ಅಂದ್ರು. \n",
      "ಆದರೆ ಕೊನೆ ಕಿರಿಮಗಳು ಮಾತ್ರ ನಾನ್‌ ಸಾಕಾಕಿಲ್ಲ ಅಂದ್ಲು. ಅಪ್ಪ ನಾವ್‌ ಸಾಕಾಕೆ ಆಗ್ಕದ? \n",
      "ಭಗವಂತ ನಮ್ಮನ್ನೆಲ್ಲ ಸಾಕ್‌ಬೇಕು. ಈ ಮಾತನ್ನೆ ಹೇಳ್ಕೊಂಡು ಬಂದ್ಲು. ಆಗ ಆ\n",
      "--- End of Preview ---\n",
      "\n",
      "--- Start of 2-neeti-kathegalu.pdf ---\n",
      "ಸ್ತ್ರೀ ನೀತಿ ಕಥೆಗಳು \n",
      "ಪುರಾತನ ಕಾಲದಲ್ಲಿ ಗೋಪಾಲಸೆ ಟ್ಟ ಎಂಬೊಬ್ಬ ವ್ಯಾಪಾರಿಯು \n",
      "ಕಾಶಿ ರ ನಗರದಲ್ಲಿ ವಾಸಿಸುತ್ತಿದ್ದರು. ಶೇಮಂತನಾದ ಈ ವರ್ತಕನಿಗೆ \n",
      "ಮೋಹಿನಿಯೊಬ್ಬ ಳೇ ಮಗಳು. ರೊಪ ಸುಂದರಿಯಾದ ಈ ಮೋಹಿನಿಯು \n",
      "ಓದುಬರಹದಲ್ಲೂ ಅಕಿ ನಿಪುಣಳು. ಅವಳು ವಯಸ್ಸಿಗೆ ಬಂದ ಕೂಡಲೇ \n",
      "ತಂದೆಯು ಮದುವೆ ಮಾಡಿಕೊಟ್ಟ ನು. ಮೋಹಿನಿಯ ಪತಿಯು ಅನುರೂಪ \n",
      "ನೇನೋ ನಿಜ, ಆದರೆ ವಿದ್ಯಾ ಬುದ್ಧಿ ಯುತ. ಸ್ವಲ್ಪ ಲ, ಮೂಢನೆಂದರೂ \n",
      "ಸರಿಯೇ. ಇದರಿಂದ ಅವಳಿಗೆ ಸುಖ ದೊರೆಯಲಿಲ್ಲ. ಅವಳು ಚಿಂತೆಯಿಂದ \n",
      "ಕೊರಗಲಾರಂಭಿಸಿದಳು. ದಿನದಿಂದ ದಿನಕ್ಕೆ ಕೃಷವಾಗುತ್ತ ಬಂದಳು. \n",
      "ಕೊನೆಗೆ ಅದೊಂದು ರೋಗವಾಗಿಯೇ ಪರಿಣಮಿಸಿತು, ಇದನ್ನು ಕಂಡ \n",
      "ಮೋಹಿನಿಯ ಗಂಡನು ಅನೇಕ ವೈದ್\n",
      "--- End of Preview ---\n"
     ]
    }
   ],
   "source": [
    "# Directory where the datasets are stored.\n",
    "repo_dir = '../kannada/kannada-dataset'\n",
    "\n",
    "extracted_texts = {}\n",
    "\n",
    "for config in pdf_configs:\n",
    "    filename = config['filename']\n",
    "    start_page = config['start']\n",
    "    end_page = config['end']\n",
    "\n",
    "    # 1. Try to find the file in the repo directory\n",
    "    file_path = os.path.join(repo_dir, filename)\n",
    "\n",
    "    # 2. If not found, check current directory\n",
    "    if not os.path.exists(file_path):\n",
    "        file_path = filename\n",
    "\n",
    "    # 3. If still invalid or corrupt, download it\n",
    "    if not os.path.exists(file_path) or os.path.getsize(file_path) < 1000:\n",
    "        print(f\"File {filename} missing or corrupt. Downloading...\")\n",
    "        url = f\"https://github.com/anagha1999/anlp-project/raw/main/kannada-dataset/{filename}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            file_path = filename\n",
    "        else:\n",
    "             print(f\"Failed to download {filename}\")\n",
    "\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text_content = []\n",
    "        # Convert 1-based page numbers to 0-based indices\n",
    "        # range(start, end) in python excludes end, so we need end_page (since we want up to end_page)\n",
    "        # Instructions say: 1-janapada-kathegalu.pdf: pages 17 to 236.\n",
    "        # Python index for page 17 is 16.\n",
    "        # We want to include page 236. So range should go up to 236 (index 235).\n",
    "        # range(16, 236) covers indices 16 to 235.\n",
    "        for i in range(start_page - 1, end_page):\n",
    "            if i < len(reader.pages):\n",
    "                page = reader.pages[i]\n",
    "                text_content.append(page.extract_text())\n",
    "\n",
    "        extracted_texts[filename] = \"\\n\".join(text_content)\n",
    "        print(f\"Successfully extracted text from {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "\n",
    "# Print the first 500 characters of each extracted text\n",
    "for filename, text in extracted_texts.items():\n",
    "    print(f\"\\n--- Start of {filename} ---\")\n",
    "    print(text[:500])\n",
    "    print(f\"--- End of Preview ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9ee8731",
    "outputId": "1decfe8d-1197-4118-a266-d1ea7fe62ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction and cleaning process completed.\n",
      "Total files processed: 2\n",
      "Saved files at:\n",
      "  kannada-pre-processed/1-janapada-kathegalu.csv\n",
      "  kannada-pre-processed/2-neeti-kathegalu.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Load stop words\n",
    "with open('stop-words.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'kannada-pre-processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cleaned_files = []\n",
    "\n",
    "# Process each extracted text\n",
    "for filename, text in extracted_texts.items():\n",
    "    # First, replace multiple consecutive newlines (paragraph breaks) with '---\\n'\n",
    "    text_with_paragraphs = re.sub(r'\\n\\s*\\n', '\\n---\\n', text)\n",
    "    \n",
    "    # Split into paragraphs\n",
    "    raw_paragraphs = text_with_paragraphs.split('---')\n",
    "    \n",
    "    # Process each paragraph to remove stop words\n",
    "    cleaned_paragraphs = []\n",
    "    for para in raw_paragraphs:\n",
    "        # Process line by line within each paragraph\n",
    "        cleaned_lines = []\n",
    "        for line in para.splitlines():\n",
    "            words = line.split()\n",
    "            cleaned_words = [w for w in words if w not in stop_words]\n",
    "            cleaned_lines.append(\" \".join(cleaned_words))\n",
    "        \n",
    "        # Join lines back and strip whitespace\n",
    "        cleaned_para = \"\\n\".join(cleaned_lines).strip()\n",
    "        if cleaned_para:  # Only add non-empty paragraphs\n",
    "            cleaned_paragraphs.append(cleaned_para)\n",
    "\n",
    "    # Determine output filename (change extension to .csv)\n",
    "    base_name = os.path.basename(filename)\n",
    "    csv_filename = os.path.splitext(base_name)[0] + \".csv\"\n",
    "    output_path = os.path.join(output_dir, csv_filename)\n",
    "\n",
    "    # Save as CSV with paragraph_num and cleaned_text columns\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['paragraph_num', 'cleaned_text'])\n",
    "        for idx, para in enumerate(cleaned_paragraphs, start=1):\n",
    "            writer.writerow([idx, para])\n",
    "\n",
    "    cleaned_files.append(output_path)\n",
    "\n",
    "print(\"Extraction and cleaning process completed.\")\n",
    "print(f\"Total files processed: {len(cleaned_files)}\")\n",
    "print(\"Saved files at:\")\n",
    "for path in cleaned_files:\n",
    "    print(f\"  {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA3539RsJn3i"
   },
   "source": [
    "## Generate Paragraph Embeddings for each Text [Kannada]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.5-cp314-cp314-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anagha/Documents/fall2025/anlp-project/.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anagha/Documents/fall2025/anlp-project/.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.5-cp314-cp314-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.3.5 pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFaS-AqpypLI",
    "outputId": "31768425-4f35-4f09-ae22-29a0ca3250cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: kannada-pre-processed/1-janapada-kathegalu.csv\n",
      "Total Paragraphs: 220\n",
      "CSV Columns: ['paragraph_num', 'cleaned_text']\n",
      "First paragraph start: ೧. ಮುತುಗದ ಎಲೆ ಗೋತಾಯಿ\n",
      "ಒಂದ್‌ ಪಟ್ಟಣ. ಒಬ್ಬ ದೊಡ್ಡ ದೊರೆ....\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the file path\n",
    "file_path_to_check = os.path.join('kannada-pre-processed', '1-janapada-kathegalu.csv')\n",
    "\n",
    "# Ensure the file exists before proceeding\n",
    "if os.path.exists(file_path_to_check):\n",
    "    # 2. Load CSV file using pandas\n",
    "    df = pd.read_csv(file_path_to_check, encoding='utf-8')\n",
    "    \n",
    "    # 3. Extract paragraphs from 'cleaned_text' column\n",
    "    paragraphs = df['cleaned_text'].dropna().tolist()\n",
    "\n",
    "    # 4. Print the count\n",
    "    print(f\"File: {file_path_to_check}\")\n",
    "    print(f\"Total Paragraphs: {len(paragraphs)}\")\n",
    "    print(f\"CSV Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    if paragraphs:\n",
    "        print(f\"First paragraph start: {paragraphs[0][:50]}...\")\n",
    "else:\n",
    "    print(f\"Error: File not found at {file_path_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uNx_fzOaKDsY"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      2\u001b[39m EMBEDDINGS_MODEL_NAME=\u001b[33m'\u001b[39m\u001b[33ml3cube-pune/indic-sentence-similarity-sbert\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m model = SentenceTransformer(EMBEDDINGS_MODEL_NAME)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "EMBEDDINGS_MODEL_NAME='l3cube-pune/indic-sentence-similarity-sbert'\n",
    "model = SentenceTransformer(EMBEDDINGS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFf6GdTUJ4rG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "target_dir = 'anlp-project/kannada/kannada-pre-processed'\n",
    "file_embeddings = {}\n",
    "\n",
    "if os.path.exists(target_dir):\n",
    "    for filename in os.listdir(target_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(target_dir, filename)\n",
    "\n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(filepath, encoding='utf-8')\n",
    "            \n",
    "            # Extract cleaned_text column\n",
    "            paragraphs = df['cleaned_text'].dropna().tolist()\n",
    "\n",
    "            if paragraphs:\n",
    "                # Generate embeddings\n",
    "                embeddings = model.encode(paragraphs)\n",
    "                file_embeddings[filename] = embeddings\n",
    "\n",
    "                print(f\"File: {filename}\")\n",
    "                print(f\"  Paragraphs processed: {len(paragraphs)}\")\n",
    "                print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "            else:\n",
    "                print(f\"File: {filename} - No paragraphs found.\")\n",
    "else:\n",
    "    print(f\"Target directory not found: {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm5c5UtrKJqd"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the file path for saving the embeddings\n",
    "embeddings_file_path = 'kannada_texts_embeddings.pkl'\n",
    "\n",
    "# Save the file_embeddings dictionary to a pickle file\n",
    "with open(embeddings_file_path, 'wb') as f:\n",
    "    pickle.dump(file_embeddings, f)\n",
    "\n",
    "print(f\"Successfully saved paragraph embeddings to {embeddings_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
